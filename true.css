import re
from typing import Dict, Any, List
import numpy as np
from numpy.linalg import norm
import requests
from langchain.prompts import PromptTemplate

class SimpleRAG:
    def __init__(self, embedding_api_url: str, llm_api_url: str):
        self.embedding_api_url = embedding_api_url
        self.llm_api_url = llm_api_url
        self.vector_store = []  # List to store document embeddings
        
        # Initialize prompt template
        self.prompt_template = PromptTemplate(
            input_variables=["context", "query"],
            template="""
            Context information: {context}
            
            User question: {query}
            
            Please provide an accurate answer based on the context information above.
            If the context doesn't contain relevant information, please say so.
            """
        )

    def pii_scan(self, text: str) -> str:
        """Simple PII scanner that identifies and masks basic PII"""
        patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
            'ssn': r'\b\d{3}-\d{2}-\d{4}\b'
        }
        
        cleaned_text = text
        for pii_type, pattern in patterns.items():
            cleaned_text = re.sub(pattern, f'[{pii_type.upper()}]', cleaned_text)
            
        return cleaned_text

    def get_embedding(self, text: str) -> List[float]:
        """Get embeddings from API"""
        response = requests.post(
            self.embedding_api_url,
            json={'text': text}
        )
        return response.json()['embedding']

    def semantic_search(self, query_embedding: List[float], top_k: int = 3) -> List[Dict]:
        """Find most similar documents using cosine similarity"""
        query_vector = np.array(query_embedding)
        
        results = []
        for doc in self.vector_store:
            similarity = np.dot(query_vector, doc['embedding']) / (
                norm(query_vector) * norm(doc['embedding'])
            )
            results.append({
                'text': doc['text'],
                'similarity': similarity
            })
        
        # Sort by similarity and return top_k
        return sorted(results, key=lambda x: x['similarity'], reverse=True)[:top_k]

    def generate_prompt(self, query: str, context_docs: List[Dict]) -> str:
        """Generate enhanced prompt using template"""
        # Format context documents
        context = "\n\n".join([doc['text'] for doc in context_docs])
        
        # Generate prompt using template
        return self.prompt_template.format(
            context=context,
            query=query
        )

    def get_llm_response(self, prompt: str) -> str:
        """Get response from LLM API"""
        response = requests.post(
            self.llm_api_url,
            json={'prompt': prompt}
        )
        return response.json()['response']

    def process_query(self, query: str) -> Dict[str, Any]:
        """Process query through the RAG pipeline"""
        # 1. PII Scan
        clean_query = self.pii_scan(query)
        
        # 2. Get query embedding
        query_embedding = self.get_embedding(clean_query)
        
        # 3. Semantic search
        similar_docs = self.semantic_search(query_embedding)
        
        # 4. Generate enhanced prompt
        enhanced_prompt = self.generate_prompt(clean_query, similar_docs)
        
        # 5. Get LLM response
        response = self.get_llm_response(enhanced_prompt)
        
        return {
            'original_query': query,
            'clean_query': clean_query,
            'similar_docs': similar_docs,
            'response': response
        }

# Example usage:
def main():
    # Initialize RAG
    rag = SimpleRAG(
        embedding_api_url="your_embedding_api_endpoint",
        llm_api_url="your_llm_api_endpoint"
    )
    
    # Add some documents to vector store (you'd typically do this during setup)
    rag.vector_store.append({
        'text': 'Example document',
        'embedding': [0.1, 0.2, 0.3]  # This would come from your embedding API
    })
    
    # Process a query
    query = "What is the capital of France?"
    result = rag.process_query(query)
    print(result['response'])

if __name__ == "__main__":
    main()