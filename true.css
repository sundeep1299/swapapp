class SimpleRAG:
    def __init__(self, db_config: Dict[str, str]):
        """
        Initialize the RAG system with database configuration.
        Args:
            db_config: Dictionary containing database connection parameters
        """
        self.db_config = db_config
        self.vectorizer = TfidfVectorizer()
        self.ai_controller = AIController()  # Initialize AI controller

    async def get_llm_response(self, query: str) -> Dict:
        """
        Get response from LLM through AI Controller
        Args:
            query: The text query to send to LLM
        Returns:
            Dictionary containing LLM response
        """
        try:
            # Create LLM payload with the query
            llm_payload = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": query
                            }
                        ],
                        "role": "model"
                    }
                ]
            }
            
            # Get response from AI Controller
            response = await self.ai_controller.call_llm()
            return response
        except Exception as e:
            logger.error(f"Error getting LLM response: {e}")
            return {"error": str(e)}

    async def process_query(self, query: str) -> Dict[str, Any]:
        """
        Process user query through the RAG pipeline.
        Args:
            query: User input query
        Returns:
            Dictionary containing processed results
        """
        # Clean and embed query
        clean_query = self.pii_scan(query)
        query_embedding = self.embed_text(clean_query)
        
        # Perform semantic search
        similar_docs = await self.semantic_search(query_embedding)
        
        # Get context from similar documents
        context = " ".join([doc["text"] for doc in similar_docs])
        
        # Combine query with context for LLM
        enhanced_query = f"Context: {context}\n\nQuery: {clean_query}"
        
        # Get LLM response
        llm_response = await self.get_llm_response(enhanced_query)
        
        # Prepare response
        return {
            'original_query': query,
            'clean_query': clean_query,
            'similar_docs': similar_docs,
            'llm_response': llm_response
        }

# Modify the main execution
if __name__ == "__main__":
    # Example configuration
    db_config = {
        "host": "localhost",
        "port": 5432,
        "database": "vectors_db",
        "user": "user",
        "password": "password"
    }
    
    # Initialize RAG system
    rag = SimpleRAG(db_config)
    
    # Example query processing
    async def run_example():
        query = "What are the key features of our product?"
        result = await rag.process_query(query)
        print(json.dumps(result, indent=2))
    
    # Run the example
    import asyncio
    asyncio.run(run_example())