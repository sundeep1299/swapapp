import re
import json
from typing import List, Dict, Any, Optional
import requests
from datetime import datetime
import logging
import numpy as np
from numpy.linalg import norm

class RAGProcessor:
    def __init__(self, embedding_api_url: str, llm_api_url: str):
        """
        Initialize RAG Processor with necessary API endpoints and vector store
        
        Args:
            embedding_api_url (str): URL for the embedding API
            llm_api_url (str): URL for the LLM API
        """
        self.embedding_api_url = embedding_api_url
        self.llm_api_url = llm_api_url
        self.logger = logging.getLogger(__name__)
        self.vector_store = []  # List to store document embeddings and metadata

    def add_to_vector_store(self, document: str, embedding: List[float], metadata: Dict[str, Any] = None):
        """
        Add a document and its embedding to the vector store
        
        Args:
            document (str): The original document text
            embedding (List[float]): The document's embedding vector
            metadata (Dict[str, Any]): Additional metadata about the document
        """
        self.vector_store.append({
            'document': document,
            'embedding': np.array(embedding),
            'metadata': metadata or {}
        })

    def cosine_similarity(self, v1: np.ndarray, v2: np.ndarray) -> float:
        """
        Calculate cosine similarity between two vectors
        
        Args:
            v1 (np.ndarray): First vector
            v2 (np.ndarray): Second vector
            
        Returns:
            float: Cosine similarity score
        """
        dot_product = np.dot(v1, v2)
        norm_product = norm(v1) * norm(v2)
        return dot_product / (norm_product + 1e-8)  # Add small epsilon to avoid division by zero

    def pii_scan(self, text: str) -> Dict[str, Any]:
        """
        Scan text for PII using regex patterns and mask/remove sensitive information
        
        Args:
            text (str): Input text to scan for PII
            
        Returns:
            Dict containing cleaned text and PII findings
        """
        pii_patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'\b(\+\d{1,2}\s?)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b',
            'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
            'credit_card': r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b',
            'date_of_birth': r'\b\d{2}/\d{2}/\d{4}\b',
            'address': r'\b\d{1,5}\s+[A-Za-z\s]{1,50}\s+(?:street|st|avenue|ave|road|rd|highway|hwy|square|sq|trail|trl|drive|dr|court|ct|park|parkway|pkwy|circle|cir|boulevard|blvd)\b',
        }

        findings = {}
        cleaned_text = text

        for pii_type, pattern in pii_patterns.items():
            matches = re.finditer(pattern, text, re.IGNORECASE)
            findings[pii_type] = []
            
            for match in matches:
                findings[pii_type].append({
                    'value': match.group(),
                    'start': match.start(),
                    'end': match.end()
                })
                # Replace PII with placeholder
                cleaned_text = cleaned_text.replace(match.group(), f"[{pii_type.upper()}]")

        return {
            'cleaned_text': cleaned_text,
            'pii_findings': findings,
            'has_pii': any(findings.values())
        }

    def generate_embedding(self, text: str) -> Dict[str, Any]:
        """
        Generate embeddings for the cleaned text using external API
        
        Args:
            text (str): Cleaned text to generate embeddings for
            
        Returns:
            Dict containing embedding vector and metadata
        """
        try:
            response = requests.post(
                self.embedding_api_url,
                json={'text': text},
                headers={'Content-Type': 'application/json'}
            )
            response.raise_for_status()
            
            return {
                'embedding': response.json()['embedding'],
                'timestamp': datetime.utcnow().isoformat(),
                'status': 'success'
            }
        except Exception as e:
            self.logger.error(f"Embedding generation failed: {str(e)}")
            return {
                'embedding': None,
                'error': str(e),
                'status': 'failed'
            }

    def semantic_search(self, query_embedding: List[float], top_k: int = 5) -> Dict[str, Any]:
        """
        Perform semantic search using vector similarity
        
        Args:
            query_embedding (List[float]): Query embedding vector
            top_k (int): Number of top results to return
            
        Returns:
            Dict containing search results and metadata
        """
        try:
            # Convert query embedding to numpy array
            query_vector = np.array(query_embedding)
            
            # Calculate similarities with all documents in vector store
            similarities = []
            for doc in self.vector_store:
                similarity = self.cosine_similarity(query_vector, doc['embedding'])
                similarities.append({
                    'document': doc['document'],
                    'similarity': similarity,
                    'metadata': doc['metadata']
                })
            
            # Sort by similarity score and get top_k results
            sorted_results = sorted(similarities, 
                                 key=lambda x: x['similarity'], 
                                 reverse=True)[:top_k]
            
            return {
                'results': sorted_results,
                'timestamp': datetime.utcnow().isoformat(),
                'status': 'success'
            }
        except Exception as e:
            self.logger.error(f"Semantic search failed: {str(e)}")
            return {
                'results': [],
                'error': str(e),
                'status': 'failed'
            }

    def enhance_prompt(self, query: str, context: List[Dict[str, Any]]) -> str:
        """
        Enhance the prompt using retrieved context and prompt engineering techniques
        
        Args:
            query (str): Original user query
            context (List[Dict]): Retrieved context passages with similarity scores
            
        Returns:
            Enhanced prompt string
        """
        prompt_template = """
        Context Information:
        {context}

        User Query: {query}

        Instructions:
        1. Use the provided context to answer the user's query
        2. If the context doesn't contain relevant information, say so
        3. Cite specific parts of the context that support your answer
        4. Be concise and specific in your response
        5. Consider the relevance scores when weighing different pieces of context

        Answer:
        """
        
        # Format context passages with similarity scores
        formatted_context = "\n\n".join([
            f"Passage {i+1} (Relevance: {ctx['similarity']:.3f}):\n{ctx['document']}"
            for i, ctx in enumerate(context)
        ])
        
        # Create enhanced prompt
        enhanced_prompt = prompt_template.format(
            context=formatted_context,
            query=query
        )
        
        return enhanced_prompt.strip()

    def get_llm_response(self, prompt: str) -> Dict[str, Any]:
        """
        Get response from LLM API using the enhanced prompt
        
        Args:
            prompt (str): Enhanced prompt to send to LLM
            
        Returns:
            Dict containing LLM response and metadata
        """
        try:
            response = requests.post(
                self.llm_api_url,
                json={
                    'prompt': prompt,
                    'max_tokens': 500,
                    'temperature': 0.7
                },
                headers={'Content-Type': 'application/json'}
            )
            response.raise_for_status()
            
            return {
                'response': response.json()['response'],
                'timestamp': datetime.utcnow().isoformat(),
                'status': 'success'
            }
        except Exception as e:
            self.logger.error(f"LLM API call failed: {str(e)}")
            return {
                'response': None,
                'error': str(e),
                'status': 'failed'
            }

    def process_query(self, query: str) -> Dict[str, Any]:
        """
        Process a complete query through the RAG pipeline
        
        Args:
            query (str): User query to process
            
        Returns:
            Dict containing complete processing results
        """
        # Step 1: PII Scan
        pii_results = self.pii_scan(query)
        clean_query = pii_results['cleaned_text']
        
        # Step 2: Generate Embedding
        embedding_results = self.generate_embedding(clean_query)
        if embedding_results['status'] == 'failed':
            return embedding_results
        
        # Step 3: Semantic Search
        search_results = self.semantic_search(embedding_results['embedding'])
        if search_results['status'] == 'failed':
            return search_results
        
        # Step 4: Enhance Prompt
        enhanced_prompt = self.enhance_prompt(clean_query, search_results['results'])
        
        # Step 5: Get LLM Response
        llm_response = self.get_llm_response(enhanced_prompt)
        
        return {
            'pii_results': pii_results,
            'embedding_results': embedding_results,
            'search_results': search_results,
            'enhanced_prompt': enhanced_prompt,
            'llm_response': llm_response,
            'timestamp': datetime.utcnow().isoformat()
        }