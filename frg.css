from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
import numpy as np
from numpy.linalg import norm
from sklearn.feature_extraction.text import TfidfVectorizer
import asyncpg
import json
import base64
import hashlib
import hmac
import time
import requests
import logging
from google.oauth2 import service_account
from google.auth.transport.requests import Request

# Configuration Classes
@dataclass
class DatabaseConfig:
    host: str
    port: int
    database: str
    user: str
    password: str

    def get_connection_params(self) -> Dict[str, Any]:
        return {
            "host": self.host,
            "port": self.port,
            "database": self.database,
            "user": self.user,
            "password": self.password
        }

@dataclass
class AuthConfig:
    app_id: str
    secret: str
    version: str
    idaas_url: str
    llm_url: str
    service_account_file: str

# Custom Exceptions
class RAGException(Exception):
    """Base exception for RAG system"""
    pass

class DatabaseConnectionError(RAGException):
    """Raised when database connection fails"""
    pass

class AuthenticationError(RAGException):
    """Raised when authentication fails"""
    pass

class LLMError(RAGException):
    """Raised when LLM interaction fails"""
    pass

# Interface Definitions
class ITokenGenerator(ABC):
    @abstractmethod
    async def generate_token(self) -> str:
        pass

class IVectorizer(ABC):
    @abstractmethod
    def vectorize(self, text: str) -> List[float]:
        pass

class IDatabase(ABC):
    @abstractmethod
    async def connect(self) -> asyncpg.Connection:
        pass

    @abstractmethod
    async def close(self) -> None:
        pass

# Implementation Classes
class Logger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self._setup_logger()

    def _setup_logger(self):
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)

class DatabaseManager(IDatabase):
    def __init__(self, config: DatabaseConfig):
        self.config = config
        self.connection: Optional[asyncpg.Connection] = None
        self.logger = Logger(__name__).logger

    async def connect(self) -> asyncpg.Connection:
        try:
            self.connection = await asyncpg.connect(**self.config.get_connection_params())
            return self.connection
        except Exception as e:
            self.logger.error(f"Database connection failed: {str(e)}")
            raise DatabaseConnectionError(f"Failed to connect to database: {str(e)}")

    async def close(self) -> None:
        if self.connection:
            await self.connection.close()

class TFIDFVectorizer(IVectorizer):
    def __init__(self):
        self.vectorizer = TfidfVectorizer()

    def vectorize(self, text: str) -> List[float]:
        X = self.vectorizer.fit_transform([text])
        return X.toarray()[0].astype(np.float32).tolist()

class PIIProcessor:
    def __init__(self):
        self.patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
            'ssn': r'\b\d{3}-\d{2}-\d{4}\b'
        }

    def remove_pii(self, text: str) -> str:
        cleaned_text = text
        for pii_type, pattern in self.patterns.items():
            cleaned_text = re.sub(pattern, f'[{pii_type.upper()}]', cleaned_text)
        return cleaned_text

class TokenManager(ITokenGenerator):
    def __init__(self, config: AuthConfig):
        self.config = config
        self.logger = Logger(__name__).logger

    async def generate_gcp_token(self) -> str:
        try:
            scopes = ["https://www.googleapis.com/auth/cloud-platform"]
            credentials = service_account.Credentials.from_service_account_file(
                self.config.service_account_file, scopes=scopes)
            credentials.refresh(Request())
            return credentials.token
        except Exception as e:
            raise AuthenticationError(f"Failed to generate GCP token: {str(e)}")

    async def generate_idaas_token(self) -> str:
        try:
            mac = hmac.new(base64.b64decode(self.config.secret), digestmod=hashlib.sha256)
            timestamp = str(int(time.time() * 1000))
            input_data = f"{self.config.app_id.strip()}-{self.config.version}-{timestamp}"
            mac.update(input_data.encode('utf-8'))
            
            hmac_signature = base64.b64encode(mac.digest()).decode('utf-8')
            hmac_signature = hmac_signature.replace('/', '_').replace('+', '-').replace('=', '')
            
            headers = {
                'Content-Type': 'application/json',
                'X-Auth-AppID': self.config.app_id,
                'X-Auth-Signature': hmac_signature,
                'X-Auth-Version': self.config.version,
                'X-Auth-Timestamp': timestamp,
                'Accept': 'application/json'
            }
            
            payload = {
                "scope": ["/genai/google/v1/models/gemini-1.5-pro-002/**:post"]
            }
            
            response = requests.post(
                self.config.idaas_url, 
                headers=headers, 
                json=payload, 
                verify=False
            )
            return response.json()['authorization_token']
        except Exception as e:
            raise AuthenticationError(f"Failed to generate IDaaS token: {str(e)}")

class LLMClient:
    def __init__(self, config: AuthConfig, token_manager: TokenManager):
        self.config = config
        self.token_manager = token_manager
        self.logger = Logger(__name__).logger

    async def generate_response(self, query: str) -> Dict[str, Any]:
        try:
            idaas_token = await self.token_manager.generate_idaas_token()
            gcp_token = await self.token_manager.generate_gcp_token()

            headers = {
                "x-gcp-authorization": f"Bearer {gcp_token}",
                "Authorization": f"Bearer {idaas_token}",
                "Content-Type": "application/json",
                "Accept": "application/json"
            }

            payload = {
                "contents": [{
                    "parts": [{"text": query}],
                    "role": "model"
                }]
            }

            response = requests.post(
                self.config.llm_url,
                headers=headers,
                json=payload,
                verify=False
            )
            
            if response.ok:
                return response.json()
            else:
                raise LLMError(f"LLM request failed: {response.text}")
        except Exception as e:
            raise LLMError(f"Failed to generate LLM response: {str(e)}")

class SemanticSearch:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.logger = Logger(__name__).logger

    async def search(self, query_vector: List[float], top_k: int = 3) -> List[Dict]:
        try:
            conn = await self.db_manager.connect()
            query = """
            SELECT id, text, embedding 
            FROM vectors_aka1128 
            WHERE embedding IS NOT NULL
            """
            rows = await conn.fetch(query)
            
            results = []
            query_np = np.array(query_vector, dtype=np.float32)
            
            for row in rows:
                try:
                    stored_embedding = self._process_embedding(row['embedding'])
                    if query_np.shape != stored_embedding.shape:
                        continue
                        
                    similarity = self._calculate_similarity(query_np, stored_embedding)
                    results.append({
                        'id': row['id'],
                        'text': row['text'],
                        'similarity': float(similarity)
                    })
                except Exception as e:
                    self.logger.error(f"Error processing row {row['id']}: {str(e)}")
                    continue
                    
            return sorted(results, key=lambda x: x['similarity'], reverse=True)[:top_k]
        finally:
            await self.db_manager.close()

    def _process_embedding(self, embedding) -> np.ndarray:
        if isinstance(embedding, str):
            embedding_str = embedding.strip('[]')
            return np.array([float(x) for x in embedding_str.split(',')], dtype=np.float32)
        return np.array(embedding, dtype=np.float32)

    def _calculate_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        numerator = np.dot(vec1, vec2)
        denominator = norm(vec1) * norm(vec2)
        return 0.0 if denominator == 0 else numerator / denominator

class RAGSystem:
    def __init__(self, 
                 db_config: DatabaseConfig,
                 auth_config: AuthConfig):
        self.db_manager = DatabaseManager(db_config)
        self.token_manager = TokenManager(auth_config)
        self.llm_client = LLMClient(auth_config, self.token_manager)
        self.vectorizer = TFIDFVectorizer()
        self.pii_processor = PIIProcessor()
        self.semantic_search = SemanticSearch(self.db_manager)
        self.logger = Logger(__name__).logger

    async def process_query(self, query: str) -> Dict[str, Any]:
        try:
            # Clean query
            clean_query = self.pii_processor.remove_pii(query)
            
            # Generate embedding
            query_embedding = self.vectorizer.vectorize(clean_query)
            
            # Perform semantic search
            similar_docs = await self.semantic_search.search(query_embedding)
            
            # Prepare context
            context = " ".join([doc["text"] for doc in similar_docs])
            enhanced_query = f"Context: {context}\n\nQuery: {clean_query}"
            
            # Get LLM response
            llm_response = await self.llm_client.generate_response(enhanced_query)
            
            return {
                'original_query': query,
                'clean_query': clean_query,
                'similar_docs': similar_docs,
                'llm_response': llm_response
            }
        except Exception as e:
            self.logger.error(f"Error processing query: {str(e)}")
            raise RAGException(f"Failed to process query: {str(e)}")

# Example usage
if __name__ == "__main__":
    # Load configurations
    db_config = DatabaseConfig(
        host="localhost",
        port=5432,
        database="vectors_db",
        user="user",
        password="password"
    )
    
    auth_config = AuthConfig(
        app_id="d1bec33a-3d0a-3c8d-a430-367cf59a52a9",
        secret="your_secret",
        version="2",
        idaas_url="https://oneidentityapi-dev.aexp.com/security/digital/v1/application/token",
        llm_url="https://pap-dev.aexp.com/genai/google/v1/models/gemini-1.5-pro-002/generateContent",
        service_account_file="path_to_your_service_account_file"
    )
    
    # Initialize RAG system
    rag_system = RAGSystem(db_config, auth_config)
    
    # Example query processing
    async def run_example():
        try:
            query = "What are the key features of our product?"
            result = await rag_system.process_query(query)
            print(json.dumps(result, indent=2))
        except RAGException as e:
            print(f"Error: {str(e)}")
    
    # Run the example
    import asyncio
    asyncio.run(run_example())